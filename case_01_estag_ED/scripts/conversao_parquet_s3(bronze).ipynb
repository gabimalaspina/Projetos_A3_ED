{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.fs import S3FileSystem\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variáveis de entrada\n",
    "nome_bucket = 'elasticbeanstalk-us-east-2-047710767346'\n",
    "subpasta = 'Case_A3'\n",
    "pasta_origem = f'{subpasta}/arquivos_extraidos'\n",
    "pasta_destino = f'{subpasta}/arquivos_extraidos_parquet'\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta Case_A3/arquivos_extraidos_parquet já existe! Caminho: s3://elasticbeanstalk-us-east-2-047710767346/Case_A3/arquivos_extraidos_parquet/\n"
     ]
    }
   ],
   "source": [
    "def verifica_ou_cria_pasta(s3, nome_bucket, pasta_destino):\n",
    "    # Verifica se a pasta de destino existe\n",
    "    try:\n",
    "        s3.head_object(Bucket=nome_bucket, Key=f'{pasta_destino}/')\n",
    "        print(f'Pasta {pasta_destino} já existe! Caminho: s3://{nome_bucket}/{pasta_destino}/')\n",
    "    except Exception as e:\n",
    "        # Se não existir, cria a pasta\n",
    "        s3.put_object(Body='', Bucket=nome_bucket, Key=f'{pasta_destino}/')\n",
    "        print(f'Pasta {pasta_destino} criada com sucesso! Caminho: s3://{nome_bucket}/{pasta_destino}/')\n",
    "verifica_ou_cria_pasta(s3, nome_bucket, pasta_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converte_csv_para_parquet(s3, nome_bucket, pasta_origem, pasta_destino):\n",
    "    # Configura o sistema de arquivos S3\n",
    "    s3_fs = S3FileSystem()\n",
    "\n",
    "    # Lista os objetos no bucket e pasta de origem\n",
    "    try:\n",
    "        objetos = s3.list_objects(Bucket=nome_bucket, Prefix=pasta_origem)\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao listar objetos em {pasta_origem}: {str(e)}')\n",
    "        return\n",
    "\n",
    "    for obj in objetos.get('Contents', []):\n",
    "        # Verifica se o objeto é uma subpasta chamada \"dados\" (sem distinção de maiúsculas e minúsculas)\n",
    "        if obj['Key'].lower() == f'{pasta_origem.lower()}dados/':\n",
    "            print(f'Processando subpasta: {obj[\"Key\"]}')\n",
    "\n",
    "            # Lista os objetos dentro da subpasta\n",
    "            try:\n",
    "                objetos_csv = s3.list_objects(Bucket=nome_bucket, Prefix=obj['Key'])\n",
    "            except Exception as e:\n",
    "                print(f'Erro ao listar objetos em {obj[\"Key\"]}: {str(e)}')\n",
    "                continue\n",
    "\n",
    "            for obj_csv in objetos_csv.get('Contents', []):\n",
    "                # Verifica se o objeto é um arquivo CSV\n",
    "                if obj_csv['Key'].lower().endswith('.csv'):\n",
    "                    print(f'Convertendo arquivo CSV: {obj_csv[\"Key\"]}')\n",
    "\n",
    "                    try:\n",
    "                        # Lê o conteúdo do arquivo CSV\n",
    "                        csv_content = s3.get_object(Bucket=nome_bucket, Key=obj_csv['Key'])['Body'].read()\n",
    "\n",
    "                        # Converte para DataFrame\n",
    "                        df = pd.read_csv(BytesIO(csv_content))\n",
    "\n",
    "                        # Converte para Parquet usando PyArrow\n",
    "                        parquet_key = obj_csv['Key'].replace(pasta_origem, pasta_destino).replace('.csv', '.parquet')\n",
    "                        \n",
    "                        print(f'Convertendo para: {parquet_key}')\n",
    "\n",
    "                        with s3_fs.open_output_stream(f'{nome_bucket}/{parquet_key}') as out_file:\n",
    "                            pq.write_table(pq.Table.from_pandas(df), out_file)\n",
    "\n",
    "                        print(f'Arquivo convertido e salvo como Parquet: {parquet_key}')\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f'Erro ao converter {obj_csv[\"Key\"]}: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chama a função\n",
    "converte_csv_para_parquet(s3, nome_bucket, pasta_origem, pasta_destino)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
